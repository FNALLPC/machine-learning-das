{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimize a dense network with Bayesian optimization\n",
    "Authors: Javier Duarte, Thong Nguyen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading `pandas` DataFrames\n",
    "Now we load two different `NumPy` arrays. One corresponding to the VV signal and one corresponding to the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "treename = 'HZZ4LeptonsAnalysisReduced'\n",
    "filename = {}\n",
    "upfile = {}\n",
    "params = {}\n",
    "df = {}\n",
    "\n",
    "filename['VV'] = 'data/ntuple_4mu_VV.root'\n",
    "filename['bkg'] = 'data/ntuple_4mu_bkg.root'\n",
    "\n",
    "VARS = ['f_mass4l','f_massjj'] # choose which vars to use (2d)\n",
    "\n",
    "upfile['VV'] = uproot.open(filename['VV'])\n",
    "params['VV'] = upfile['VV'][treename].arrays(VARS)\n",
    "upfile['bkg'] = uproot.open(filename['bkg'])\n",
    "params['bkg'] = upfile['bkg'][treename].arrays(VARS)\n",
    "\n",
    "df['VV'] = pd.DataFrame(params['VV'],columns=VARS)\n",
    "df['bkg'] = pd.DataFrame(params['bkg'],columns=VARS)\n",
    "\n",
    "# cut out undefined variables VARS[0] and VARS[1] > -999\n",
    "df['VV']= df['VV'][(df['VV'][VARS[0]] > -999) & (df['VV'][VARS[1]] > -999)]\n",
    "df['bkg']= df['bkg'][(df['bkg'][VARS[0]] > -999) & (df['bkg'][VARS[1]] > -999)] \n",
    "\n",
    "# add isSignal variable\n",
    "df['VV']['isSignal'] = np.ones(len(df['VV'])) \n",
    "df['bkg']['isSignal'] = np.zeros(len(df['bkg'])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "We'll start with a dense (fully-connected) NN layer.\n",
    "Our model will have a single fully-connected hidden layer with the same number of neurons as input variables. \n",
    "The weights are initialized using a small Gaussian random number. \n",
    "We will switch between linear and tanh activation functions for the hidden layer.\n",
    "The output layer contains a single neuron in order to make predictions. \n",
    "It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1.\n",
    "\n",
    "We are using the `binary_crossentropy` loss function during training, a standard loss function for binary classification problems. \n",
    "We will optimize the model with the Adam algorithm for stochastic gradient descent and we will collect accuracy metrics while the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline keras model\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Input, Activation, Dense, Convolution2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras.utils import np_utils\n",
    "from keras import optimizers\n",
    "\n",
    "NDIM = len(VARS)\n",
    "inputs = Input(shape=(NDIM,), name = 'input')  \n",
    "outputs = Dense(1, name = 'output', kernel_initializer='normal', activation='sigmoid')(inputs)\n",
    "\n",
    "# creae the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dividing the data into testing and training dataset\n",
    "\n",
    "We will split the data into two parts (one for training+validation and one for testing). \n",
    "We will also apply \"standard scaling\" preprocessing: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html i.e. making the mean = 0 and the RMS = 1 for all input variables (based **only** on the training/validation dataset).\n",
    "We will also define our early stopping criteria to prevent over-fitting and we will save the model based on the best `val_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.concat([df['VV'],df['bkg']])\n",
    "dataset = df_all.values\n",
    "X = dataset[:,0:NDIM]\n",
    "Y = dataset[:,NDIM]\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "\n",
    "# preprocessing: standard scalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train_val)\n",
    "X_train_val = scaler.transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=10)\n",
    "\n",
    "# model checkpoint callback\n",
    "# this saves our model architecture + parameters into dense_model.h5\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "model_checkpoint = ModelCheckpoint('dense_model.h5', monitor='val_loss', \n",
    "                                   verbose=0, save_best_only=True, \n",
    "                                   save_weights_only=False, mode='auto', \n",
    "                                   period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training \n",
    "Here, we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "import time\n",
    "begt = time.time()\n",
    "history = model.fit(X_train_val, \n",
    "                    Y_train_val, \n",
    "                    epochs=100, \n",
    "                    batch_size=1024, \n",
    "                    verbose=0, # switch to 1 for more verbosity \n",
    "                    callbacks=[early_stopping, model_checkpoint], \n",
    "                    validation_split=0.25)\n",
    "print(\"Finished in {}s\".format(time.time()-begt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize the hyperparameters of the model\n",
    "The hyperperparameters of the model that we weill optimize are the number of hidden layers `num_hidden`, the number of nodes in each layer `initial_node`, and the fraction of dropout `dropout`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-optimize --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "\n",
    "def build_custom_model(num_hiddens=2, initial_node=50, \n",
    "                          dropout=0.5):\n",
    "    inputs = Input(shape=(NDIM,), name = 'input')\n",
    "    for i in range(num_hiddens):\n",
    "        hidden = Dense(int(round(initial_node/np.power(2,i))), activation='relu')(inputs if i==0 else hidden)\n",
    "        hidden = Dropout(np.float32(dropout))(hidden)\n",
    "    outputs = Dense(1, name = 'output', kernel_initializer='normal', activation='sigmoid')(hidden)\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train(batch_size=1000):\n",
    "    history = model.fit(X_train_val, \n",
    "                    Y_train_val, \n",
    "                    epochs=100, \n",
    "                    batch_size=batch_size, \n",
    "                    verbose=0, # switch to 1 for more verbosity \n",
    "                    callbacks=[early_stopping, model_checkpoint], \n",
    "                    validation_split=0.25)\n",
    "    best_acc = max(history.history['val_acc'])\n",
    "    return best_acc\n",
    "\n",
    "space  = [Integer(1, 3, name='hidden_layers'),\n",
    "          Integer(5, 100, name='initial_nodes'),\n",
    "          Real(0.0,0.9,name='dropout'),\n",
    "          Integer(500,5000,name='batch_size'),\n",
    "          Real(10**-5, 10**-1, \"log-uniform\", name='learning_rate'),\n",
    "          ]\n",
    "\n",
    "@use_named_args(space)\n",
    "def objective(**X):\n",
    "    print(\"New configuration: {}\".format(X))\n",
    "\n",
    "    model = build_custom_model(num_hiddens=X['hidden_layers'], initial_node=X['initial_nodes'], \n",
    "                      dropout=X['dropout'])\n",
    "\n",
    "    model.compile(optimizer=optimizers.Adam(lr=X['learning_rate']), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    model.summary()\n",
    "\n",
    "    best_acc = train(batch_size=X['batch_size'])\n",
    "        \n",
    "    print(\"Best acc: {}\".format(best_acc))\n",
    "    return -best_acc\n",
    "\n",
    "begt = time.time()\n",
    "res_gp = gp_minimize(objective, space, n_calls=20, random_state=3)\n",
    "print(\"Finish optimization in {}s\".format(time.time()-begt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the improvement\n",
    "Let's see how Bayesian optimization improves the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skopt.plots import plot_convergence\n",
    "plot_convergence(res_gp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Best parameters: \\\n",
    "\\nbest_hidden_layers = {} \\\n",
    "\\nbest_initial_nodes = {} \\\n",
    "\\nbest_dropout = {} \\\n",
    "\\nbest_batch_size = {} \\\n",
    "\\nbest_learning_rate = {}\".format(res_gp.x[0],\n",
    "                                 res_gp.x[1],\n",
    "                                 res_gp.x[2],\n",
    "                                 res_gp.x[3],\n",
    "                                 res_gp.x[4]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance\n",
    "Here, we plot the history of the training and the performance in a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# plot loss vs epoch\n",
    "plt.figure(figsize=(15,10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.plot(history.history['loss'], label='loss')\n",
    "ax.plot(history.history['val_loss'], label='val_loss')\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('loss')\n",
    "\n",
    "# plot accuracy vs epoch\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.plot(history.history['acc'], label='acc')\n",
    "ax.plot(history.history['val_acc'], label='val_acc')\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlabel('epoch')\n",
    "ax.set_ylabel('acc')\n",
    "\n",
    "# Plot ROC\n",
    "Y_predict = model.predict(X_test)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_predict)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.plot(fpr, tpr, lw=2, color='cyan', label='auc = %.3f' % (roc_auc))\n",
    "ax.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='random chance')\n",
    "ax.set_xlim([0, 1.0])\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.set_xlabel('false positive rate')\n",
    "ax.set_ylabel('true positive rate')\n",
    "ax.set_title('receiver operating curve')\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-das",
   "language": "python",
   "name": "machine-learning-das"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
