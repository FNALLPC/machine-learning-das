{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dense neural network with PyTorch\n",
    "Authors: Javier Duarte, Tyler Mitchell "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install torch torchvision root_pandas --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import root_pandas\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy TTree HZZ4LeptonsAnalysisReduced into a pandas DataFrame.\n",
    "filename = {\n",
    "    'VV': root_pandas.read_root('data/ntuple_4mu_VV.root', key='HZZ4LeptonsAnalysisReduced'),\n",
    "    'bkg': root_pandas.read_root('data/ntuple_4mu_bkg.root', key='HZZ4LeptonsAnalysisReduced')\n",
    "}\n",
    "\n",
    "# Drop all variables except for those we want to use when training.\n",
    "VARS = ['f_mass4l','f_massjj']\n",
    "df = {\n",
    "    'VV': filename['VV'][VARS],\n",
    "    'bkg': filename['bkg'][VARS],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the inputs are well behaved.\n",
    "df['VV']= df['VV'][(df['VV'][VARS[0]] > -999) & (df['VV'][VARS[1]] > -999)]\n",
    "df['bkg']= df['bkg'][(df['bkg'][VARS[0]] > -999) & (df['bkg'][VARS[1]] > -999)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add isSignal variable\n",
    "df['VV']['isSignal'] = np.ones(len(df['VV'])) \n",
    "df['bkg']['isSignal'] = np.zeros(len(df['bkg'])) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine signal and background into one DataFrame then split into input variables and labels.\n",
    "NDIM = len(VARS)\n",
    "df_all = pd.concat([df['VV'],df['bkg']])\n",
    "dataset = df_all.values\n",
    "X = dataset[:,0:NDIM]\n",
    "Y = dataset[:,NDIM]\n",
    "\n",
    "# Split into training and testing data.\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "print(X_train_val)\n",
    "print(X)\n",
    "\n",
    "# preprocessing: standard scalar (reshape inputs to mean=0, variance=1)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler().fit(X_train_val)\n",
    "X_train_val = scaler.transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Split again, this time into training and validation data.\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X_train_val, Y_train_val, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build our model. \n",
    "import torch\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(2, 20),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 20),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(20, 1),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "# Use Binary Cross Entropy as our loss function.\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "\n",
    "# Optimize the model parameters using the Adam optimizer.\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getBatches(N, tot):\n",
    "    \"\"\"Function to find nearest acceptable batch size\"\"\"\n",
    "    print N, tot\n",
    "    if tot % N == 0:\n",
    "        return N\n",
    "    closest = 1\n",
    "    for i in xrange(1, N):\n",
    "        if tot % i == 0 and abs(N - i) < abs(N-closest):\n",
    "            closest = i\n",
    "    return closest\n",
    "    \n",
    "# Create batches from total data. We have to make sure the batch size is an appropriate divisor of the total \n",
    "# number of training events\n",
    "N = getBatches(1024, len(X_train))\n",
    "unbatched_x = torch.from_numpy(X_train).float()\n",
    "x = unbatched_x.view(-1, N, len(VARS))\n",
    "unbatched_y = torch.from_numpy(Y_train).float()\n",
    "y_b = unbatched_y.view(-1, N, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation data ready\n",
    "val_data = torch.from_numpy(X_val).float()\n",
    "val_label = torch.from_numpy(Y_val).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses, val_losses = [], []\n",
    "min_loss, stale_epochs = 100., 0\n",
    "\n",
    "# 500 epochs. \n",
    "for t in xrange(500):\n",
    "    batch_loss, val_batch_loss = [], []\n",
    "    for b in xrange(len(x)):\n",
    "        \n",
    "        # Forward pass: make a prediction for each x event in batch b.\n",
    "        y_pred = model(x[b])\n",
    "\n",
    "        # Get the labels.\n",
    "        label = y_b[b]\n",
    "        y = label.view_as(y_pred)  # reshape label data to the shape of y_pred\n",
    "\n",
    "        # Compute and print loss.\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        batch_loss.append(loss.item())\n",
    "\n",
    "        # Let's look at the validation set.\n",
    "        \n",
    "        # Torch keeps track of each operation performed on a Tensor, so that it can take the gradient later.\n",
    "        # We don't need to store this information when looking at validation data, so turn it off with\n",
    "        # torch.no_grad().\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Forward pass on validation set.\n",
    "            output = model(val_data)\n",
    "\n",
    "            # Get labels and compute loss again\n",
    "            val_y = val_label.view_as(output)\n",
    "            val_loss = loss_fn(output, val_y)\n",
    "            val_batch_loss.append(val_loss.item())\n",
    "\n",
    "            # Monitor the loss function to prevent overtraining.\n",
    "            if stale_epochs > 20:\n",
    "                break\n",
    "\n",
    "            if val_loss.item() - min_loss < 0:\n",
    "                min_loss = val_loss.item()\n",
    "                stale_epochs = 0\n",
    "            else:\n",
    "                stale_epochs += 1\n",
    "\n",
    "        #print(t, b, loss.item(), val_loss.item())\n",
    "\n",
    "        # Before the backward pass, use the optimizer object to zero all of the\n",
    "        # gradients for the variables it will update (which are the learnable\n",
    "        # weights of the model). This is because by default, gradients are\n",
    "        # accumulated in buffers( i.e, not overwritten) whenever .backward()\n",
    "        # is called. Checkout docs of torch.autograd.backward for more details.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Backward pass: compute gradient of the loss with respect to model\n",
    "        # parameters\n",
    "        loss.backward()\n",
    "\n",
    "        # Calling the step function on an Optimizer makes an update to its\n",
    "        # parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "    losses.append(min(batch_loss))\n",
    "    val_losses.append(min(val_batch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "with torch.no_grad():\n",
    "    # plot loss vs epoch\n",
    "    plt.figure(figsize=(16,8))\n",
    "    ax = plt.subplot(1, 2, 1)\n",
    "    ax.plot(losses, label='loss')\n",
    "    ax.plot(val_losses, label='val_loss')\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Plot ROC\n",
    "    X_test_in = torch.from_numpy(X_test).float()\n",
    "    Y_predict = model(X_test_in)\n",
    "    from sklearn.metrics import roc_curve, auc\n",
    "    fpr, tpr, thresholds = roc_curve(Y_test, Y_predict)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(16,8))\n",
    "    ax2 = plt.subplot(1, 2, 2)\n",
    "    ax2.plot(fpr, tpr, lw=2, color='cyan', label='auc = %.3f' % (roc_auc))\n",
    "    ax2.plot([0, 1], [0, 1], linestyle='--', lw=2, color='k', label='random chance')\n",
    "    ax2.set_xlim([0, 1.0])\n",
    "    ax2.set_ylim([0, 1.0])\n",
    "    ax2.set_xlabel('false positive rate')\n",
    "    ax2.set_ylabel('true positive rate')\n",
    "    ax2.set_title('receiver operating curve')\n",
    "    ax2.legend(loc=\"lower right\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: We want to make a three 2D numpy arrays: \n",
    "# x values at each (x, y) grid point\n",
    "# y values at each (x, y) grid point\n",
    "# z values (model prediction) at each (x, y) grid point\n",
    "\n",
    "myXI, myYI = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))\n",
    "# print shape\n",
    "print(myXI.shape)\n",
    "\n",
    "myZI = np.zeros(myXI.shape)\n",
    "\n",
    "for i in range(0, len(myXI)):\n",
    "    for j in range(0, len(myYI)):\n",
    "        myXI[i,j] # x value of xi, yj point\n",
    "        myYI[i,j] # y value of xi, yj point\n",
    "        data_point = torch.tensor([myXI[i,j], myYI[i,j]])\n",
    "        myZI[i,j] = model(data_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(myZI)\n",
    "myZI = model(torch.from_numpy(np.c_[myXI.ravel(), myYI.ravel()]).float())\n",
    "myZI = myZI.reshape(myXI.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "plt.figure(figsize=(20,7))\n",
    "\n",
    "# plot contour map of NN output\n",
    "# overlaid with test data points\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "\n",
    "cont_plot = ax.contourf(myXI, myYI, myZI>0.5, cmap=cm, alpha=.8)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, edgecolors='k')\n",
    "ax.set_xlim(-2,2)\n",
    "ax.set_ylim(-2,2)\n",
    "ax.set_xlabel(VARS[0])\n",
    "ax.set_ylabel(VARS[1])\n",
    "plt.colorbar(cont_plot,ax=ax, boundaries=[0,1],label='NN output')\n",
    "\n",
    "# plot decision boundary\n",
    "# overlaid with test data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python39",
   "language": "python",
   "name": "python39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
