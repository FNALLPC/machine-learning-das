{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing jet-images\n",
    "Author: Javier Duarte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download `NumPy` array files\n",
    "The files you will need are available at `root://cmseos.fnal.gov//store/user/cmsdas/2021/short_exercises/MachineLearning/`. Make sure you have obtained a valid proxy before proceeding. If you have not installed your grid certificate on this machine or are otherwise unable to copy the files via XRootD, you may copy them from a local area. Please be aware that the local copy is not actively maintained and cannot be garanteed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "local=False\n",
    "exist = (len(glob.glob('data/TT')) > 0) & (len(glob.glob('data/QCD_*to*')) > 0)\n",
    "if not exist:\n",
    "    if local:\n",
    "        !cp -r /home/cms.aperloff/machine-learning-das-2019/data/TT/ data/\n",
    "        !cp -r /home/cms.aperloff/machine-learning-das-2019/data/QCD_120to170/ data/\n",
    "        !cp -r /home/cms.aperloff/machine-learning-das-2019/data/QCD_170to300/ data/\n",
    "        !cp -r /home/cms.aperloff/machine-learning-das-2019/data/QCD_300to470/ data/\n",
    "        !cp -r /home/cms.aperloff/machine-learning-das-2019/data/QCD_470to600/ data/\n",
    "    else:\n",
    "        !xrdcp -r root://cmseos.fnal.gov//store/user/cmsdas/2021/short_exercises/MachineLearning/TT/ data/\n",
    "        !xrdcp -r root://cmseos.fnal.gov//store/user/cmsdas/2021/short_exercises/MachineLearning/QCD_120to170/ data/\n",
    "        !xrdcp -r root://cmseos.fnal.gov//store/user/cmsdas/2021/short_exercises/MachineLearning/QCD_170to300/ data/\n",
    "        !xrdcp -r root://cmseos.fnal.gov//store/user/cmsdas/2021/short_exercises/MachineLearning/QCD_300to470/ data/\n",
    "        !xrdcp -r root://cmseos.fnal.gov//store/user/cmsdas/2021/short_exercises/MachineLearning/QCD_470to600/ data/\n",
    "else:\n",
    "    print \"The input files already exist in the 'data' directory!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load `NumPy` array files\n",
    "Here we load the `NumPy` array files.\n",
    "\n",
    "If you're using an image with less than 8GB or memory, then you should lower the 'max_file' parameter. For a 2GB image we recommend a value of 3 or lower.\n",
    "\n",
    "If you're still having problems loading many files (i.e. the kernel keeps dying), set the 'debug' variable to 'True', which will install a memory profiler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install and turn on a memory profiler\n",
    "debug = False\n",
    "if debug:\n",
    "    import sys\n",
    "    !{sys.executable} -m pip install memory_profiler --user\n",
    "    %load_ext memory_profiler\n",
    "\n",
    "# get input numpy arrays\n",
    "inputs = {}\n",
    "inputs['TT'] = glob.glob('data/TT/*job*.npy')\n",
    "inputs['QCD120'] = glob.glob('data/QCD_120to170/*job*.npy')\n",
    "inputs['QCD170'] = glob.glob('data/QCD_170to300/*job*.npy')\n",
    "inputs['QCD300'] = glob.glob('data/QCD_300to470/*job*.npy')\n",
    "inputs['QCD470'] = glob.glob('data/QCD_470to600/*job*.npy')\n",
    "\n",
    "fraction_file = 0.3\n",
    "max_file = 22\n",
    "\n",
    "params = {}\n",
    "for key, input_files in inputs.iteritems():\n",
    "    list_params = {}\n",
    "    list_params[key] = []\n",
    "    print key,len(input_files),\"files\"\n",
    "    last_file = int(len(input_files)*fraction_file)+1 if fraction_file>0 else -1\n",
    "    last_file = min(max_file, len(input_files)) if max_file else last_file\n",
    "    #if key=='TT': last_file*=20\n",
    "    print \"Getting\",last_file,\"/\",len(input_files),\"files\"\n",
    "    for in_file in input_files[:last_file]:\n",
    "        try:\n",
    "            print in_file\n",
    "            arr = np.load(in_file)\n",
    "            list_params[key].append(arr)\n",
    "        except ValueError:\n",
    "            print 'bad file: %s'%in_file\n",
    "        except IOError:\n",
    "            print 'bad io',in_file\n",
    "    if debug:\n",
    "        %memit params[key] = np.concatenate(list_params[key])\n",
    "    else:\n",
    "        params[key] = np.concatenate(list_params[key])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to `pandas` DataFrames\n",
    "Here, we convert to `pandas` DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pt_min = 200\n",
    "pt_max = 500\n",
    "\n",
    "df_dict_jet = {}\n",
    "df_dict_jet['TT'] = pd.DataFrame(params['TT'],\n",
    "                                 columns=['run', 'lumi', 'event', 'met', 'sumet', 'rho', 'pthat', \n",
    "                                          'mcweight', 'njet_ak7', 'jet_pt_ak7', 'jet_eta_ak7', \n",
    "                                          'jet_phi_ak7', 'jet_E_ak7', 'jet_msd_ak7', 'jet_area_ak7', \n",
    "                                          'jet_jes_ak7', 'jet_tau21_ak7', 'jet_isW_ak7', 'jet_ncand_ak7',\n",
    "                                          'ak7pfcand_ijet'])\n",
    "df_dict_jet['TT'] = df_dict_jet['TT'].drop_duplicates()\n",
    "df_dict_jet['TT'] =  df_dict_jet['TT'][(df_dict_jet['TT'].jet_pt_ak7 > pt_min) & (df_dict_jet['TT'].jet_pt_ak7 < pt_max) &  (df_dict_jet['TT'].jet_isW_ak7==1)]\n",
    "\n",
    "df_dict_cand = {}\n",
    "df_dict_cand['TT'] = pd.DataFrame(params['TT'],\n",
    "                                  columns=['event', 'jet_pt_ak7', 'jet_isW_ak7', 'ak7pfcand_pt', 'ak7pfcand_eta', 'ak7pfcand_phi', 'ak7pfcand_id', 'ak7pfcand_charge', 'ak7pfcand_ijet'])\n",
    "df_dict_cand['TT'] =  df_dict_cand['TT'][(df_dict_cand['TT'].jet_pt_ak7 > pt_min) & (df_dict_cand['TT'].jet_pt_ak7 < pt_max) &  (df_dict_cand['TT'].jet_isW_ak7==1)]\n",
    "\n",
    "print 'number of W jets: %i'%len(df_dict_jet['TT'])\n",
    "\n",
    "for QCDbin in ['QCD120','QCD170','QCD300','QCD470']:\n",
    "    df_dict_jet[QCDbin] = pd.DataFrame(params[QCDbin],columns=['run', 'lumi', 'event', 'met', 'sumet', 'rho', 'pthat', \n",
    "                                                               'mcweight', 'njet_ak7', 'jet_pt_ak7', 'jet_eta_ak7', \n",
    "                                                               'jet_phi_ak7', 'jet_E_ak7', 'jet_msd_ak7', 'jet_area_ak7', \n",
    "                                                               'jet_jes_ak7', 'jet_tau21_ak7', 'jet_isW_ak7', 'jet_ncand_ak7',\n",
    "                                                               'ak7pfcand_ijet'])\n",
    "    df_dict_jet[QCDbin] = df_dict_jet[QCDbin].drop_duplicates()\n",
    "    df_dict_jet[QCDbin] =  df_dict_jet[QCDbin][(df_dict_jet[QCDbin].jet_pt_ak7 > pt_min) & (df_dict_jet[QCDbin].jet_pt_ak7 < 500) &  (df_dict_jet[QCDbin].jet_isW_ak7==0)]\n",
    "    # take every 20th jet just to make the training faster and have a sample roughly the size of W jets\n",
    "    df_dict_jet[QCDbin] = df_dict_jet[QCDbin].iloc[::20, :]\n",
    "    \n",
    "    df_dict_cand[QCDbin] = pd.DataFrame(params[QCDbin],columns=['event', 'jet_pt_ak7', 'jet_isW_ak7', 'ak7pfcand_pt', 'ak7pfcand_eta', 'ak7pfcand_phi', 'ak7pfcand_id', 'ak7pfcand_charge', 'ak7pfcand_ijet'])\n",
    "    df_dict_cand[QCDbin] =  df_dict_cand[QCDbin][(df_dict_cand[QCDbin].jet_pt_ak7 > 200) & (df_dict_cand[QCDbin].jet_pt_ak7 < 500) &  (df_dict_cand[QCDbin].jet_isW_ak7==0)]\n",
    "    \n",
    "    print 'number of QCD jets in bin %s: %i'%( QCDbin, len(df_dict_jet[QCDbin]))\n",
    "    \n",
    "df_dict_jet['QCD'] = pd.concat([df_dict_jet['QCD120'],df_dict_jet['QCD170'],df_dict_jet['QCD300'],df_dict_jet['QCD470']])\n",
    "df_dict_cand['QCD'] = pd.concat([df_dict_cand['QCD120'],df_dict_cand['QCD170'],df_dict_cand['QCD300'],df_dict_cand['QCD470']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for rotation and preprocessing jet images\n",
    "Here, we define functions for rotation and preprocessing jet images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotation + (possible) reflection needed later\n",
    "def rotate_and_reflect(x,y,w):\n",
    "    rot_x = []\n",
    "    rot_y = []\n",
    "    theta = 0\n",
    "    maxPt = -1\n",
    "    for ix, iy, iw in zip(x, y, w):\n",
    "        dv = np.matrix([[ix],[iy]])-np.matrix([[x.iloc[0]],[y.iloc[0]]])\n",
    "        dR = np.linalg.norm(dv)\n",
    "        thisPt = iw\n",
    "        if dR > 0.35 and thisPt > maxPt:\n",
    "            maxPt = thisPt\n",
    "            # rotation in eta-phi plane c.f  https://arxiv.org/abs/1407.5675 and https://arxiv.org/abs/1511.05190:\n",
    "            # theta = -np.arctan2(iy,ix)-np.radians(90)\n",
    "            # rotation by lorentz transformation c.f. https://arxiv.org/abs/1704.02124:\n",
    "            px = iw * np.cos(iy)\n",
    "            py = iw * np.sin(iy)\n",
    "            pz = iw * np.sinh(ix)\n",
    "            theta = np.arctan2(py,pz)+np.radians(90)\n",
    "            \n",
    "    c, s = np.cos(theta), np.sin(theta)\n",
    "    R = np.matrix('{} {}; {} {}'.format(c, -s, s, c))\n",
    "    for ix, iy, iw in zip(x, y, w):\n",
    "        # rotation in eta-phi plane:\n",
    "        #rot = R*np.matrix([[ix],[iy]])\n",
    "        #rix, riy = rot[0,0], rot[1,0]\n",
    "        # rotation by lorentz transformation\n",
    "        px = iw * np.cos(iy)\n",
    "        py = iw * np.sin(iy)\n",
    "        pz = iw * np.sinh(ix)\n",
    "        rot = R*np.matrix([[py],[pz]])\n",
    "        px1 = px\n",
    "        py1 = rot[0,0]\n",
    "        pz1 = rot[1,0]\n",
    "        iw1 = np.sqrt(px1*px1+py1*py1)\n",
    "        rix, riy = np.arcsinh(pz1/iw1), np.arcsin(py1/iw1)\n",
    "        rot_x.append(rix)\n",
    "        rot_y.append(riy)\n",
    "        \n",
    "    # now reflect if leftSum > rightSum\n",
    "    leftSum = 0\n",
    "    rightSum = 0\n",
    "    for ix, iy, iw in zip(x, y, w):\n",
    "        if ix > 0: \n",
    "            rightSum += iw\n",
    "        elif ix < 0:\n",
    "            leftSum += iw\n",
    "    if leftSum > rightSum:\n",
    "        ref_x = [-1.*rix for rix in rot_x]\n",
    "        ref_y = rot_y\n",
    "    else:\n",
    "        ref_x = rot_x\n",
    "        ref_y = rot_y\n",
    "    \n",
    "    return np.array(ref_x), np.array(ref_y)\n",
    "\n",
    "import matplotlib as mpl\n",
    "import keras.backend as K\n",
    "\n",
    "nx = 30 # size of image in eta\n",
    "ny = 30 # size of image in phi\n",
    "xbins = np.linspace(-1.4,1.4,nx+1)\n",
    "ybins = np.linspace(-1.4,1.4,ny+1)\n",
    "\n",
    "def prepareImages(process, df_dict_cand, df_dict_jet):\n",
    "    list_x = []\n",
    "    list_y = []\n",
    "    list_w = []\n",
    "    if K.image_dim_ordering()=='tf':\n",
    "        jet_images = np.zeros((len(df_dict_jet[process]), nx, ny, 1))\n",
    "    else:        \n",
    "        jet_images = np.zeros((len(df_dict_jet[process]), 1, nx, ny))\n",
    "    \n",
    "    for i in range(0,len(df_dict_jet[process])):\n",
    "        # get the ith jet\n",
    "        df_dict_cand_i = df_dict_cand[process][(df_dict_cand[process]['ak7pfcand_ijet'] == df_dict_jet[process]['ak7pfcand_ijet'].iloc[i]) & (df_dict_cand[process]['event'] == df_dict_jet[process]['event'].iloc[i]) ]\n",
    "        # relative eta\n",
    "        x = df_dict_cand_i['ak7pfcand_eta']-df_dict_cand_i['ak7pfcand_eta'].iloc[0]\n",
    "        # relative phi\n",
    "        y = df_dict_cand_i['ak7pfcand_phi']-df_dict_cand_i['ak7pfcand_phi'].iloc[0]\n",
    "        weights = df_dict_cand_i['ak7pfcand_pt'] # pt of candidate is the weight\n",
    "        x,y = rotate_and_reflect(x,y,weights)\n",
    "        list_x.append(x)\n",
    "        list_y.append(y)\n",
    "        list_w.append(weights)\n",
    "        hist, xedges, yedges = np.histogram2d(x, y, weights=weights, bins=(xbins,ybins))\n",
    "        for ix in range(0,nx):\n",
    "            for iy in range(0,ny):\n",
    "                if K.image_dim_ordering()=='tf':\n",
    "                    jet_images[i,ix,iy,0] = hist[ix,iy]\n",
    "                else:\n",
    "                    jet_images[i,0,ix,iy] = hist[ix,iy]\n",
    "    return jet_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare jet images\n",
    "Here, we run the pre-defined functions to prepare the jet images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jet_images_dict = {}\n",
    "# 4D tensor (tensorflow backend)\n",
    "# 1st dim is jet index\n",
    "# 2nd dim is eta bin\n",
    "# 3rd dim is phi bin\n",
    "# 4th dim is pt value (or rgb layer, etc.)\n",
    "\n",
    "jet_images_dict['TT'] = prepareImages('TT', df_dict_cand, df_dict_jet)\n",
    "jet_images_dict['QCD'] = prepareImages('QCD', df_dict_cand, df_dict_jet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write jet images to `HDF5` file as `NumPy` arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "h5f = h5py.File(\"data/jet_images.h5\", \"w\")\n",
    "h5f.create_dataset('QCD', data=jet_images_dict['QCD'], compression='lzf')\n",
    "h5f.create_dataset('TT', data=jet_images_dict['TT'], compression='lzf')\n",
    "h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine-learning-das",
   "language": "python",
   "name": "machine-learning-das"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
